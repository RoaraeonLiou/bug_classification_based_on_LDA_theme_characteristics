{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gensim\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index2vector(word2index_file, index2vectorex2vec_out, dim, scale, seed=0):\n",
    "    print(\"index2vec\", word2index_file)\n",
    "    \"\"\"加载word2index文件\"\"\"\n",
    "    word2index = pickle.load(open(word2index_file, \"rb\"))\n",
    "    \"\"\"定义词汇表大小\"\"\"\n",
    "    vocab_size = len(word2index)\n",
    "    \"\"\"定义索引-向量的列表，并初始化为0向量\"\"\"\n",
    "    index2vec = np.zero((vocab_size + 1, dim), dtype=\"float32\")\n",
    "    \"\"\"将代表padding的向量设为0向量\"\"\"\n",
    "    index2vec[0] = np.zero(dim)\n",
    "    \"\"\"设置随机数seed\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \"\"\"对所有的词汇设置随机向量\"\"\"\n",
    "    for word in word2index:\n",
    "        index = word2index[word]\n",
    "        index2vec[index] = np.random.uniform(-scale, scale, dim)\n",
    "    \"\"\"存储到文件\"\"\"\n",
    "    pickle.dump(np.asarray(idnex2vec), open(index2vec_out, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index2vector_pretained(word2index_file, index2vec_out, dim, scale, seed=0):\n",
    "    print(\"index2vector_pretained\", word2index_file)\n",
    "    \"\"\"加载谷歌预训练集\"\"\"\n",
    "    word_emb = gensim.models.KeyedVectors.load_word2vec_format(\"D:\\\\BaiduNetdiskDownload\\\\GoogleNews-vectors-negative300\\\\GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "    \"\"\"加载word2index文件\"\"\"\n",
    "    word2index = pickle.load(open(word2index_file, \"rb\"))\n",
    "    \"\"\"定义词汇表大小\"\"\"\n",
    "    vocab_size = len(word2index)\n",
    "    \"\"\"定义索引-向量的列表，并初始化为0向量\"\"\"\n",
    "    index2vec = np.zeros((vocab_size + 1, dim), dtype=\"float32\")\n",
    "    \"\"\"将代表padding的向量设为0向量\"\"\"\n",
    "    index2vec[0] = np.zeros(dim)  # 0 used for padding\n",
    "    \"\"\"设置随机数seed\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \"\"\"未知词汇记录\"\"\"\n",
    "    unknown_words = 0\n",
    "    \"\"\"对所有的词汇使用预训练集进行初始化，未知词汇随机初始化\"\"\"\n",
    "    for word in word2index:\n",
    "        index = word2index[word]\n",
    "        try:\n",
    "            cur_vec = word_emb.get_vector(word)\n",
    "        except Exception:\n",
    "            \"\"\"未知词汇采用随机初始化，并计数加一\"\"\"\n",
    "            cur_vec = np.random.uniform(-scale, scale, dim)\n",
    "            unknown_words += 1\n",
    "        index2vec[index] = cur_vec\n",
    "    print(\"total words : \", vocab_size)\n",
    "    print(\"unknown words : \", unknown_words)\n",
    "    \"\"\"存储到文件\"\"\"\n",
    "    pickle.dump(np.asarray(index2vec), open(index2vec_out, \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_forward(in_file, out_file, data_length):\n",
    "    print(\"pad_forward\", in_file)\n",
    "    nn_data = []\n",
    "    \"\"\"加载输入文件\"\"\"\n",
    "    for data in pickle.load(open(in_file, 'rb')):\n",
    "        print(data)\n",
    "        sent = data[1]\n",
    "        if len(sent) >= data_length: # 截断\n",
    "            sent = sent[:data_length]\n",
    "        else: # 填充\n",
    "            pad = [0] * (data_length - len(sent))\n",
    "            sent = pad + sent\n",
    "        nn_data.append([data[0], sent, data[2]])\n",
    "    pickle.dump(nn_data, open(out_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_project(project_name):\n",
    "    if not os.path.exists(project_name):\n",
    "        os.mkdir(project_name)\n",
    "    \n",
    "    \"\"\"读文件\"\"\"\n",
    "    input_file = open(\"dataset/\" + project_name + \".pkl\", \"rb\")\n",
    "    info = pickle.load(input_file)\n",
    "    input_file.close()\n",
    "    \n",
    "    \"\"\"创建列表\"\"\"\n",
    "    key = list()\n",
    "    key_id = list()\n",
    "    summary = list()\n",
    "    description = list()\n",
    "    \n",
    "    \"\"\"收集数据\"\"\"\n",
    "    for i in range(len(info)):\n",
    "        key.append(info[i]['key'])\n",
    "        cur_index = info[i]['key'].find(\"-\")\n",
    "        key_id.append(int(info[i]['key'][cur_index+1:]))\n",
    "        summary.append(info[i]['fields']['summary'])\n",
    "        description.append(info[i]['fields']['description'])\n",
    "    \n",
    "    \"\"\"转换为numpy矩阵\"\"\"\n",
    "    key = np.array(key)\n",
    "    key_id = np.array(key_id)\n",
    "    summary = np.array(summary)\n",
    "    description = np.array(description)\n",
    "    \n",
    "    \"\"\"元素从小到大排序后提取缩索引\"\"\"\n",
    "    indicies = np.argsort(key_id)\n",
    "    key = key[indicies]\n",
    "    summary = summary[indicies]\n",
    "    description = description[indicies]\n",
    "    \n",
    "    \"\"\"合并description和summary，并分词\"\"\"\n",
    "    summary_descriptions = list()\n",
    "    for i in range(len(summary)):\n",
    "        cur_summary = summary[i]\n",
    "        cur_description = description[i]\n",
    "        summary_words = list(word_tokenize(cur_summary))\n",
    "        summary_words = [word.lower() for word in summary_words]\n",
    "        try:\n",
    "            description_words = list(word_tokenize(cur_description))\n",
    "            description_words = [word.lower() for word in description_words]\n",
    "        except Exception:\n",
    "            description_words = list()\n",
    "        summary_descriptions.append(summary_words+description_words)\n",
    "    summary_descriptions = np.array(summary_descriptions)\n",
    "    \n",
    "    \"\"\"将数字和.换为<NUM>\"\"\"\n",
    "    processed_summary_descriptions = []\n",
    "    pattern_1 = re.compile(\"^[0-9\\.]+$\")\n",
    "    for i in range(len(summary_descriptions)):\n",
    "        cur_summary_description = []\n",
    "        for word in summary_descriptions[i]:\n",
    "            if pattern_1.match(word) is not None:\n",
    "                cur_summary_description.append(\"<NUM>\")\n",
    "                continue\n",
    "            cur_summary_description.append(word)\n",
    "        processed_summary_descriptions.append(cur_summary_description)\n",
    "    processed_summary_descriptions = np.array(processed_summary_descriptions)\n",
    "    \n",
    "    \"\"\"读取label并整理为1/0\"\"\"\n",
    "    info = pd.read_csv(\"dataset/\" + project_name + \"_classification_vs_type.csv\")\n",
    "    label = list((info['CLASSIFIED'] == 'BUG').astype(int))\n",
    "    label = np.array(label)\n",
    "    label = label[indicies]\n",
    "    \n",
    "    \"\"\"将整合后的summary_description转换为索引序列\"\"\"\n",
    "    word2index = {} # 单词-索引映射表\n",
    "    index_label = list() # 记录所有文档，element eg：list[文档id, 文档内单词索引序列, 标签]\n",
    "    index = 1 # 0用于padding\n",
    "    for i in range(len(processed_summary_descriptions)):\n",
    "        sent_index = [] # 用于记录当前文档内单词索引的序列\n",
    "        for word in processed_summary_descriptions[i]:\n",
    "            if word not in word2index:\n",
    "                word2index[word] = index\n",
    "                sent_index.append(index)\n",
    "                index+=1\n",
    "            else:\n",
    "                sent_index.append(word2index[word])\n",
    "        index_label.append([key[i], sent_index, label[i]])\n",
    "        \n",
    "    \"\"\"将word2index、index_label写入文件\"\"\"\n",
    "    pickle.dump(word2index, open(project_name + \"/word2index.pkl\", 'wb'))\n",
    "    pickle.dump(index_label, open(project_name + \"/index_label.pkl\", 'wb'))\n",
    "    \n",
    "    \"\"\"词向量化，并进行padding操作\"\"\"\n",
    "    index2vector(project_name + \"/word2index.pkl\", project_name + \"/index2vec.pkl\", dim=100, scale=0.1)\n",
    "    index2vector_pretained(project_name + \"/word2index.pkl\", project_name + \"/index2vec_pt.pkl\", dim=300, scale=0.1)\n",
    "    pad_forward(project_name + \"/index_label.pkl\", project_name + \"/index_label_nn.pkl\", data_length=100)\n",
    "\n",
    "    \"\"\"加载经过padding的文件\"\"\"\n",
    "    input_file = open(project_name + \"/index_label_nn.pkl\", \"rb\")\n",
    "    data = pickle.load(input_file)\n",
    "    input_file.close()\n",
    "    \n",
    "    \"\"\"划分训练集、验证集、测试集\"\"\"\n",
    "    train_valid_data = data[:int(len(data) * 0.9)] # 90%的数据用于训练和验证\n",
    "    train_valid_data = shuffle(train_valid_data, random_state=0) # shuffle\n",
    "    train_data = train_valid_data[:int(len(train_valid_data) * 0.9)] # 81%的数据用于训练\n",
    "    valid_data = train_valid_data[int(len(train_valid_data) * 0.9):] # 9%的数据用于验证\n",
    "    test_data = data[int(len(data) * 0.9):] # 其余10%的数据用于测试\n",
    "    \"\"\"将三个数据集存储到文件\"\"\"\n",
    "    pickle.dump(train_data, open(project_name + \"/train_nn.pkl\", 'wb'))\n",
    "    pickle.dump(valid_data, open(project_name + \"/valid_nn.pkl\", 'wb'))\n",
    "    pickle.dump(test_data, open(project_name + \"/test_nn.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all():\n",
    "    if not os.path.exists(\"all\"):\n",
    "        os.mkdir(\"all\")\n",
    "\n",
    "    word2index = {}\n",
    "    index_label = []\n",
    "    index = 1  # 0 used for padding\n",
    "\n",
    "    for project_name in [\"jackrabbit\", \"lucene\", \"httpclient\"]:\n",
    "        # 打开文件\n",
    "        input_file = open(\"dataset/\" + project_name + \".pkl\", \"rb\")\n",
    "        info = pickle.load(input_file)\n",
    "        input_file.close()\n",
    "\n",
    "        # 提取key、summary、description\n",
    "        key = []\n",
    "        summary = []\n",
    "        description = []\n",
    "        for i in range(len(info)):\n",
    "            key.append(info[i]['key'])\n",
    "            summary.append(info[i]['fields']['summary'])\n",
    "            description.append(info[i]['fields']['description'])\n",
    "\n",
    "        # 组合summary和description\n",
    "        summary_descriptions = []\n",
    "        for i in range(len(summary)):\n",
    "            cur_summary = summary[i]\n",
    "            cur_description = description[i]\n",
    "            summary_words = list(word_tokenize(cur_summary))\n",
    "            summary_words = [word.lower() for word in summary_words]\n",
    "            try:\n",
    "                description_words = list(word_tokenize(cur_description))\n",
    "                description_words = [word.lower() for word in description_words]\n",
    "            except Exception:\n",
    "                description_words = []\n",
    "            summary_descriptions.append(summary_words + description_words)\n",
    "        summary_descriptions = np.array(summary_descriptions)\n",
    "\n",
    "        # 将summary_description中的数字和.转换为<NUM>标识符\n",
    "        processed_summary_descriptions = []\n",
    "        pattern1 = re.compile(\"^[0-9\\.]+$\")\n",
    "        for i in range(len(summary_descriptions)):\n",
    "            cur_summary_description = []\n",
    "            for word in summary_descriptions[i]:\n",
    "                if pattern1.match(word) is not None:\n",
    "                    cur_summary_description.append(\"<NUM>\")\n",
    "                    continue\n",
    "                cur_summary_description.append(word)\n",
    "            processed_summary_descriptions.append(cur_summary_description)\n",
    "        processed_summary_descriptions = np.array(processed_summary_descriptions)\n",
    "\n",
    "        # 从文件中提取出标签label并标记为0/1\n",
    "        info = pd.read_csv(\"dataset/\" + project_name + \"_classification_vs_type.csv\")\n",
    "        label = list((info['CLASSIFIED'] == \"BUG\").astype(int))\n",
    "        \n",
    "        # 将单词映射到索引\n",
    "        for i in range(len(processed_summary_descriptions)):\n",
    "            sent_index = []\n",
    "            for word in processed_summary_descriptions[i]:\n",
    "                if word not in word2index:\n",
    "                    word2index[word] = index # 将新的索引添加到总表\n",
    "                    sent_index.append(index)\n",
    "                    index += 1\n",
    "                else:\n",
    "                    sent_index.append(word2index[word])\n",
    "            index_label.append([key[i], sent_index, label[i]]) # 将新的文档信息添加到总表\n",
    "    # 将word2index和index_label存储到文件\n",
    "    pickle.dump(word2index, open(\"all/word2index.pkl\", 'wb'))\n",
    "    pickle.dump(index_label, open(\"all/index_label.pkl\", 'wb'))\n",
    "\n",
    "    # 词向量话和padding操作\n",
    "    index2vector(\"all/word2index.pkl\", \"all/index2vec.pkl\", dim=100, scale=0.1)\n",
    "    index2vector_pretained(\"all/word2index.pkl\", \"all/index2vec_pt.pkl\", dim=300, scale=0.1)\n",
    "    pad_forward(\"all/index_label.pkl\", \"all/index_label_nn.pkl\", data_length=100)\n",
    "\n",
    "    # 加载经过padding的文件\n",
    "    input_file = open(\"all/index_label_nn.pkl\", \"rb\")\n",
    "    data = pickle.load(input_file)\n",
    "    input_file.close()\n",
    "    \n",
    "    # 提取出所有的key_id, 并按递增顺序进行排序\n",
    "    info = []\n",
    "    for project_name in ['jackrabbit', 'lucene', 'httpclient']:\n",
    "        input_file = open(\"dataset/\" + project_name + \".pkl\", \"rb\")\n",
    "        info += pickle.load(input_file)\n",
    "        input_file.close()\n",
    "    key_id = []\n",
    "    for i in range(len(info)):\n",
    "        key_id.append(info[i]['id'])\n",
    "    key_id = np.array(key_id).astype(int)\n",
    "    indicies = np.argsort(key_id)\n",
    "    \n",
    "    # 划分训练集、验证集、测试集\n",
    "    train_valid_indicies = indicies[:int(len(indicies) * 0.9)] # 90%用于训练和验证\n",
    "    test_indicies = indicies[int(len(indicies) * 0.9):] # 10%用于测试\n",
    "    train_valid_data = [data[i] for i in train_valid_indicies] # 分离出训练验证集\n",
    "    test_data = [data[i] for i in test_indicies] # 分离出测试集\n",
    "    train_valid_data = shuffle(train_valid_data, random_state=0) # 打乱训练验证集\n",
    "    train_data = train_valid_data[:int(len(train_valid_data) * 0.95)] # 85.5%用于训练\n",
    "    valid_data = train_valid_data[int(len(train_valid_data) * 0.95):] # 其余4.5%用于验证\n",
    "    # 存储到文件\n",
    "    pickle.dump(train_data, open(\"all/train_nn.pkl\", 'wb'))\n",
    "    pickle.dump(valid_data, open(\"all/valid_nn.pkl\", 'wb'))\n",
    "    pickle.dump(test_data, open(\"all/test_nn.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"preprocess jackrabbit\")\n",
    "    preprocess_project(\"jackrabbit\")\n",
    "    print(\"preprocess lucene\")\n",
    "    preprocess_project(\"lucene\")\n",
    "    print(\"preprocess httpclient\")\n",
    "    preprocess_project(\"httpclient\")\n",
    "    print(\"preprocess all\")\n",
    "    preprocess_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
