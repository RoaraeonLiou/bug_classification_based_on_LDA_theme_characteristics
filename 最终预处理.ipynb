{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gensim\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#分别对summary和description建立词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'output666'\n",
    "stop_words_path = 'dataset/english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(inputs, stop_words_path):\n",
    "    '''\n",
    "    去除停用词\n",
    "    :param inputs: [word1 word2...]\n",
    "    :param stop_words_path:\n",
    "    :return:\n",
    "    '''\n",
    "    with open(stop_words_path, \"r\", encoding=\"utf-8\") as fr:\n",
    "        stop_words = [line.strip() for line in fr.readlines()]\n",
    "\n",
    "    outputs = [word for word in inputs if word not in stop_words]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(inputs):\n",
    "    '''\n",
    "    词干化处理\n",
    "    :param inputs: [word1 word2...]\n",
    "    :return:\n",
    "    '''\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    # stemmer = PorterStemmer()\n",
    "    outputs = [stemmer.stem(word) for word in inputs]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_forward(in_file, out_file, summary_len, description_len):\n",
    "    '''对词向量进行截取或者填充'''\n",
    "    print(\"pad_forward\", in_file)\n",
    "    nn_data = []\n",
    "    for data in pickle.load(open(in_file, 'rb')):\n",
    "        summary_sent = data[1]\n",
    "        description_sent = data[2]\n",
    "        if len(summary_sent) >= summary_len:\n",
    "            summary_sent = summary_sent[:summary_len]\n",
    "        else:\n",
    "            pad = [0] * (summary_len - len(summary_sent))\n",
    "            summary_sent = pad + summary_sent\n",
    "\n",
    "        if len(description_sent) >= description_len:\n",
    "            description_sent = description_sent[:description_len]\n",
    "        else:\n",
    "            pad = [0] * (description_len - len(description_sent))\n",
    "            description_sent = pad + description_sent\n",
    "        nn_data.append([data[0], summary_sent, description_sent, data[3], data[4]])\n",
    "    pickle.dump(nn_data, open(out_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_project(project_name):\n",
    "\n",
    "    project_dir = output_dir + '\\\\' + project_name\n",
    "    if not os.path.exists(project_dir):\n",
    "        os.makedirs(project_dir)\n",
    "\n",
    "    #读取pkl文件\n",
    "    input_file = open(\"dataset/\" + project_name + \".pkl\", \"rb\")\n",
    "    info = pickle.load(input_file)\n",
    "    input_file.close()\n",
    "\n",
    "    #处理数据\n",
    "    key = []\n",
    "    key_id = []\n",
    "    summary = []\n",
    "    description = []\n",
    "    priority = []\n",
    "    for i in range(len(info)):\n",
    "        key.append(info[i]['key']) #'HTTPCLIENT-569'\n",
    "        cur_index = info[i]['key'].find(\"-\") #10\n",
    "        key_id.append(int(info[i]['key'][cur_index + 1:])) #569\n",
    "        summary.append(info[i]['fields']['summary'])\n",
    "        description.append(info[i]['fields']['description'])\n",
    "        priority.append(info[i]['fields']['priority']['id'])\n",
    "    key = np.array(key)\n",
    "    key_id = np.array(key_id)\n",
    "    summary = np.array(summary)\n",
    "    description = np.array(description)\n",
    "    priority = np.array(priority)\n",
    "    \"\"\"\n",
    "    key: 软件缺陷报告的key\n",
    "    key_id：软件缺陷报告的key_id\n",
    "    summary：软件缺陷报告的摘要\n",
    "    description：件缺陷报告的描述\n",
    "    priority：软件缺陷报告的优先级\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"将所有元素按照key_id升序进行排序\"\"\"\n",
    "    indicies = np.argsort(key_id) #元素从小到大排列后提取索引\n",
    "    key = key[indicies]\n",
    "    summary = summary[indicies]\n",
    "    description = description[indicies]\n",
    "    priority = priority[indicies]\n",
    "\n",
    "    #将summary、description分别分词、去停用词\n",
    "    summary_tokenize = []\n",
    "    description_tokenize = []\n",
    "    for i in range(len(summary)):\n",
    "        cur_summary = summary[i]\n",
    "        cur_description = description[i]\n",
    "        \"\"\"处理摘要\"\"\"\n",
    "        summary_words = list(word_tokenize(cur_summary)) # 分词\n",
    "        summary_words = [word.lower() for word in summary_words] # 转换为小写\n",
    "        summary_words = remove_stop_words(summary_words, stop_words_path) # 去除停用词\n",
    "        summary_words = stem_words(summary_words) # 词干化\n",
    "        try:\n",
    "            \"\"\"处理描述\"\"\"\n",
    "            description_words = list(word_tokenize(cur_description)) # 分词\n",
    "            description_words = [word.lower() for word in description_words] # 转换为小写\n",
    "            description_words = remove_stop_words(description_words, stop_words_path) # 去除停用词\n",
    "            description_words = stem_words(description_words) # 词干化\n",
    "        except Exception:\n",
    "            description_words = []\n",
    "        \"\"\"将处理后的摘要和描述放入切分词列表\"\"\"\n",
    "        summary_tokenize.append(summary_words)\n",
    "        description_tokenize.append(description_words)\n",
    "    summary_processed = np.array(summary_tokenize)\n",
    "    description_processed = np.array(description_tokenize)\n",
    "\n",
    "\n",
    "    #读取label并整理 classified为bug的值为1，否则为0\n",
    "    info = pd.read_csv(\"dataset/\" + project_name + \"_classification_vs_type.csv\")\n",
    "    label = list((info['CLASSIFIED'] == \"BUG\").astype(int))\n",
    "    label = np.array(label)\n",
    "    label = label[indicies]\n",
    "\n",
    "    word2index_summary = {} # word: index\n",
    "    word2index_description = {} # 单词到索引的映射\n",
    "    \n",
    "    index_label = [] #[key,[summary_index...],[description_index...],label]\n",
    "    index_summary = 1  # 0 used for padding\n",
    "    index_description = 1\n",
    "    for i in range(len(summary_processed)):\n",
    "        summary_index = []\n",
    "        description_index = []\n",
    "\n",
    "        for word in summary_processed[i]:\n",
    "            if word not in word2index_summary:\n",
    "                word2index_summary[word] = index_summary\n",
    "                summary_index.append(index_summary)\n",
    "                index_summary += 1\n",
    "            else:\n",
    "                summary_index.append(word2index_summary[word])\n",
    "\n",
    "        for word in description_processed[i]:\n",
    "            if word not in word2index_description:\n",
    "                word2index_description[word] = index_description\n",
    "                description_index.append(index_description)\n",
    "                index_description += 1\n",
    "            else:\n",
    "                description_index.append(word2index_description[word])\n",
    "\n",
    "        index_label.append([key[i], summary_index, description_index, priority[i], label[i]])\n",
    "\n",
    "    print(len(word2index_summary))\n",
    "    print(len(word2index_description))\n",
    "    print(len(index_label))\n",
    "    #将word2index、index_label写入文件\n",
    "    pickle.dump(word2index_summary, open(project_dir + \"/word2index_summary.pkl\", 'wb'))\n",
    "    pickle.dump(word2index_description, open(project_dir + \"/word2index_description.pkl\", 'wb'))\n",
    "    pickle.dump(index_label, open(project_dir + \"/index_label.pkl\", 'wb'))\n",
    "\n",
    "    # 对句向量进行截取或填充\n",
    "    pad_forward(project_dir + \"/index_label.pkl\", project_dir + \"/index_label_nn.pkl\",\n",
    "                summary_len=50, description_len=100)\n",
    "\n",
    "    #将整理好的数据划分为训练集、验证集、测试集\n",
    "    input_file = open(project_dir + \"/index_label_nn.pkl\", \"rb\")\n",
    "    data = pickle.load(input_file)\n",
    "    input_file.close()\n",
    "    train_valid_data = data[:int(len(data) * 0.9)]\n",
    "    train_valid_data = shuffle(train_valid_data, random_state=0)\n",
    "    train_data = train_valid_data[:int(len(train_valid_data) * 0.9)]\n",
    "    valid_data = train_valid_data[int(len(train_valid_data) * 0.9):]\n",
    "    test_data = data[int(len(data) * 0.9):]\n",
    "    print('train:',len(train_data))\n",
    "    print('validation', len(valid_data))\n",
    "    print('test', len(test_data))\n",
    "    pickle.dump(train_data, open(project_dir + \"/train_nn.pkl\", 'wb'))\n",
    "    pickle.dump(valid_data, open(project_dir + \"/valid_nn.pkl\", 'wb'))\n",
    "    pickle.dump(test_data, open(project_dir + \"/test_nn.pkl\", 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all():\n",
    "\n",
    "    all_dir = output_dir + '\\\\' + 'all'\n",
    "    if not os.path.exists(all_dir):\n",
    "        os.makedirs(all_dir)\n",
    "\n",
    "    word2index_summary = {}  # word: index\n",
    "    word2index_description = {}\n",
    "    index_label = []\n",
    "    index_summary = 1  # 0 used for padding\n",
    "    index_description = 1\n",
    "    print('t1')\n",
    "\n",
    "    for project_name in [\"jackrabbit\", \"lucene\", \"httpclient\"]:\n",
    "        print('t2')\n",
    "        input_file = open(\"dataset/\" + project_name + \".pkl\", \"rb\")\n",
    "        info = pickle.load(input_file)\n",
    "        input_file.close()\n",
    "\n",
    "        key = []\n",
    "        summary = []\n",
    "        description = []\n",
    "        priority = []\n",
    "        for i in range(len(info)):\n",
    "            key.append(info[i]['key'])\n",
    "            summary.append(info[i]['fields']['summary'])\n",
    "            description.append(info[i]['fields']['description'])\n",
    "            priority.append(info[i]['fields']['priority']['id'])\n",
    "\n",
    "        # 将summary、description分别分词、去停用词\n",
    "        summary_tokenize = []\n",
    "        description_tokenize = []\n",
    "        for i in range(len(summary)):\n",
    "            cur_summary = summary[i]\n",
    "            cur_description = description[i]\n",
    "            summary_words = list(word_tokenize(cur_summary))\n",
    "            summary_words = [word.lower() for word in summary_words]\n",
    "            summary_words = remove_stop_words(summary_words, stop_words_path)\n",
    "            summary_words = stem_words(summary_words)\n",
    "            try:\n",
    "                description_words = list(word_tokenize(cur_description))\n",
    "                description_words = [word.lower() for word in description_words]\n",
    "                description_words = remove_stop_words(description_words, stop_words_path)\n",
    "                description_words = stem_words(description_words)\n",
    "            except Exception:\n",
    "                description_words = []\n",
    "            summary_tokenize.append(summary_words)\n",
    "            description_tokenize.append(description_words)\n",
    "        summary_processed = np.array(summary_tokenize)\n",
    "        description_processed = np.array(description_tokenize)\n",
    "\n",
    "        info = pd.read_csv(\"dataset/\" + project_name + \"_classification_vs_type.csv\")\n",
    "        label = list((info['CLASSIFIED'] == \"BUG\").astype(int))\n",
    "\n",
    "        for i in range(len(summary_processed)):\n",
    "            summary_index = []\n",
    "            description_index = []\n",
    "\n",
    "            for word in summary_processed[i]:\n",
    "                if word not in word2index_summary:\n",
    "                    word2index_summary[word] = index_summary\n",
    "                    summary_index.append(index_summary)\n",
    "                    index_summary += 1\n",
    "                else:\n",
    "                    summary_index.append(word2index_summary[word])\n",
    "\n",
    "            for word in description_processed[i]:\n",
    "                if word not in word2index_description:\n",
    "                    word2index_description[word] = index_description\n",
    "                    description_index.append(index_description)\n",
    "                    index_description += 1\n",
    "                else:\n",
    "                    description_index.append(word2index_description[word])\n",
    "\n",
    "            index_label.append([key[i], summary_index, description_index, priority[i], label[i]])\n",
    "\n",
    "    # 将word2index、index_label写入文件\n",
    "    pickle.dump(word2index_summary, open(all_dir + \"/word2index_summary.pkl\", 'wb'))\n",
    "    pickle.dump(word2index_description, open(all_dir + \"/word2index_description.pkl\", 'wb'))\n",
    "    pickle.dump(index_label, open(all_dir + \"/index_label.pkl\", 'wb'))\n",
    "\n",
    "    # 对句向量进行截取或填充\n",
    "    pad_forward(all_dir + \"/index_label.pkl\", all_dir + \"/index_label_nn.pkl\",\n",
    "                summary_len=50, description_len=100)\n",
    "\n",
    "    # 将整理好的数据划分为训练集、验证集、测试集\n",
    "    input_file = open(all_dir + \"/index_label_nn.pkl\", \"rb\")\n",
    "    data = pickle.load(input_file)\n",
    "    input_file.close()\n",
    "\n",
    "    info = []\n",
    "    for project_name in ['jackrabbit', 'lucene', 'httpclient']:\n",
    "        input_file = open(\"dataset/\" + project_name + \".pkl\", \"rb\")\n",
    "        info += pickle.load(input_file)\n",
    "        input_file.close()\n",
    "\n",
    "    key_id = []\n",
    "    for i in range(len(info)):\n",
    "        key_id.append(info[i]['id'])\n",
    "    key_id = np.array(key_id).astype(int)\n",
    "    indicies = np.argsort(key_id)\n",
    "\n",
    "    train_valid_indicies = indicies[:int(len(indicies) * 0.9)]\n",
    "    test_indicies = indicies[int(len(indicies) * 0.9):]\n",
    "\n",
    "    train_valid_data = [data[i] for i in train_valid_indicies]\n",
    "    test_data = [data[i] for i in test_indicies]\n",
    "\n",
    "    train_valid_data = shuffle(train_valid_data, random_state=0)\n",
    "    train_data = train_valid_data[:int(len(train_valid_data) * 0.95)]\n",
    "    valid_data = train_valid_data[int(len(train_valid_data) * 0.95):]\n",
    "\n",
    "    pickle.dump(train_data, open(all_dir + \"/train_nn.pkl\", 'wb'))\n",
    "    pickle.dump(valid_data, open(all_dir + \"/valid_nn.pkl\", 'wb'))\n",
    "    pickle.dump(test_data, open(all_dir + \"/test_nn.pkl\", 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess lucene\n",
      "3385\n",
      "18619\n",
      "2443\n",
      "pad_forward output666\\lucene/index_label.pkl\n",
      "train: 1978\n",
      "validation 220\n",
      "test 245\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # print(\"preprocess jackrabbit\")\n",
    "    # preprocess_project(\"jackrabbit\")\n",
    "    print(\"preprocess lucene\")\n",
    "    preprocess_project(\"lucene\")\n",
    "    # print(\"preprocess httpclient\")\n",
    "    # preprocess_project(\"httpclient\")\n",
    "#     print(\"preprocess all\")\n",
    "#     preprocess_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
