{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gensim\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#分别对summary和description建立词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'Model Input'\n",
    "stop_words_path = 'dataset/english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(inputs, stop_words_path):\n",
    "    '''\n",
    "    去除停用词\n",
    "    :param inputs: [word1 word2...]\n",
    "    :param stop_words_path:\n",
    "    :return:\n",
    "    '''\n",
    "    with open(stop_words_path, \"r\", encoding=\"utf-8\") as fr:\n",
    "        stop_words = [line.strip() for line in fr.readlines()]\n",
    "\n",
    "    outputs = [word for word in inputs if word not in stop_words]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(inputs):\n",
    "    '''\n",
    "    词干化处理\n",
    "    :param inputs: [word1 word2...]\n",
    "    :return:\n",
    "    '''\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    # stemmer = PorterStemmer()\n",
    "    outputs = [stemmer.stem(word) for word in inputs]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_forward(in_file, out_file, summary_len, description_len):\n",
    "    '''对词向量进行截取或者填充'''\n",
    "    print(\"pad_forward\", in_file)\n",
    "    nn_data = []\n",
    "    for data in pickle.load(open(in_file, 'rb')):\n",
    "        summary_sent = data[1]\n",
    "        description_sent = data[2]\n",
    "        if len(summary_sent) >= summary_len:\n",
    "            summary_sent = summary_sent[:summary_len]\n",
    "        else:\n",
    "            pad = [0] * (summary_len - len(summary_sent))\n",
    "            summary_sent = pad + summary_sent\n",
    "\n",
    "        if len(description_sent) >= description_len:\n",
    "            description_sent = description_sent[:description_len]\n",
    "        else:\n",
    "            pad = [0] * (description_len - len(description_sent))\n",
    "            description_sent = pad + description_sent\n",
    "        nn_data.append([data[0], summary_sent, description_sent, data[3], data[4]])\n",
    "    pickle.dump(nn_data, open(out_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_project(project_name):\n",
    "\n",
    "    project_dir = output_dir + '\\\\' + project_name\n",
    "    if not os.path.exists(project_dir):\n",
    "        os.makedirs(project_dir)\n",
    "\n",
    "    #读取pkl文件\n",
    "    input_file = open(\"dataset/\" + project_name + \".pkl\", \"rb\")\n",
    "    info = pickle.load(input_file)\n",
    "    input_file.close()\n",
    "\n",
    "    #处理数据\n",
    "    key = []\n",
    "    key_id = []\n",
    "    summary = []\n",
    "    description = []\n",
    "    priority = []\n",
    "    for i in range(len(info)):\n",
    "        key.append(info[i]['key']) #'HTTPCLIENT-569'\n",
    "        cur_index = info[i]['key'].find(\"-\") #10\n",
    "        key_id.append(int(info[i]['key'][cur_index + 1:])) #569\n",
    "        summary.append(info[i]['fields']['summary'])\n",
    "        description.append(info[i]['fields']['description'])\n",
    "        priority.append(info[i]['fields']['priority']['id'])\n",
    "    key = np.array(key)\n",
    "    key_id = np.array(key_id)\n",
    "    summary = np.array(summary)\n",
    "    description = np.array(description)\n",
    "    priority = np.array(priority)\n",
    "    \"\"\"\n",
    "    key: 软件缺陷报告的key\n",
    "    key_id：软件缺陷报告的key_id\n",
    "    summary：软件缺陷报告的摘要\n",
    "    description：件缺陷报告的描述\n",
    "    priority：软件缺陷报告的优先级\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"将所有元素按照key_id升序进行排序\"\"\"\n",
    "    indicies = np.argsort(key_id) #元素从小到大排列后提取索引\n",
    "    key = key[indicies]\n",
    "    summary = summary[indicies]\n",
    "    description = description[indicies]\n",
    "    priority = priority[indicies]\n",
    "\n",
    "    #将summary、description分别分词、去停用词\n",
    "    summary_tokenize = []\n",
    "    description_tokenize = []\n",
    "    for i in range(len(summary)):\n",
    "        cur_summary = summary[i]\n",
    "        cur_description = description[i]\n",
    "        \"\"\"处理摘要\"\"\"\n",
    "        summary_words = list(word_tokenize(cur_summary)) # 分词\n",
    "        summary_words = [word.lower() for word in summary_words] # 转换为小写\n",
    "        summary_words = remove_stop_words(summary_words, stop_words_path) # 去除停用词\n",
    "        summary_words = stem_words(summary_words) # 词干化\n",
    "        try:\n",
    "            \"\"\"处理描述\"\"\"\n",
    "            description_words = list(word_tokenize(cur_description)) # 分词\n",
    "            description_words = [word.lower() for word in description_words] # 转换为小写\n",
    "            description_words = remove_stop_words(description_words, stop_words_path) # 去除停用词\n",
    "            description_words = stem_words(description_words) # 词干化\n",
    "        except Exception:\n",
    "            description_words = []\n",
    "        \"\"\"将处理后的摘要和描述放入切分词列表\"\"\"\n",
    "        summary_tokenize.append(summary_words)\n",
    "        description_tokenize.append(description_words)\n",
    "    summary_processed = np.array(summary_tokenize)\n",
    "    description_processed = np.array(description_tokenize)\n",
    "\n",
    "\n",
    "    #读取label并整理 classified为bug的值为1，否则为0\n",
    "    info = pd.read_csv(\"dataset/\" + project_name + \"_classification_vs_type.csv\")\n",
    "    label = list((info['CLASSIFIED'] == \"BUG\").astype(int))\n",
    "    label = np.array(label)\n",
    "    label = label[indicies]\n",
    "\n",
    "    word2index_summary = {} # word: index\n",
    "    word2index_description = {} # 单词到索引的映射\n",
    "    \n",
    "    index_label = [] #[key,[summary_index...],[description_index...],label]\n",
    "    index_summary = 1  # 0 used for padding\n",
    "    index_description = 1\n",
    "    for i in range(len(summary_processed)):\n",
    "        summary_index = []\n",
    "        description_index = []\n",
    "\n",
    "        for word in summary_processed[i]:\n",
    "            if word not in word2index_summary:\n",
    "                word2index_summary[word] = index_summary\n",
    "                summary_index.append(index_summary)\n",
    "                index_summary += 1\n",
    "            else:\n",
    "                summary_index.append(word2index_summary[word])\n",
    "\n",
    "        for word in description_processed[i]:\n",
    "            if word not in word2index_description:\n",
    "                word2index_description[word] = index_description\n",
    "                description_index.append(index_description)\n",
    "                index_description += 1\n",
    "            else:\n",
    "                description_index.append(word2index_description[word])\n",
    "\n",
    "        index_label.append([key[i], summary_index, description_index, priority[i], label[i]])\n",
    "\n",
    "    print(len(word2index_summary))\n",
    "    print(len(word2index_description))\n",
    "    print(len(index_label))\n",
    "    #将word2index、index_label写入文件\n",
    "    pickle.dump(word2index_summary, open(project_dir + \"/word2index_summary.pkl\", 'wb'))\n",
    "    pickle.dump(word2index_description, open(project_dir + \"/word2index_description.pkl\", 'wb'))\n",
    "    pickle.dump(index_label, open(project_dir + \"/index_label.pkl\", 'wb'))\n",
    "\n",
    "    # 对句向量进行截取或填充\n",
    "    pad_forward(project_dir + \"/index_label.pkl\", project_dir + \"/index_label_nn.pkl\",\n",
    "                summary_len=50, description_len=100)\n",
    "\n",
    "    #将整理好的数据划分为训练集、验证集、测试集\n",
    "    input_file = open(project_dir + \"/index_label_nn.pkl\", \"rb\")\n",
    "    data = pickle.load(input_file)\n",
    "    input_file.close()\n",
    "    \n",
    "    theme_file = open(\"LDA theme/\" + project_name + \"_theme.pkl\", \"rb\")\n",
    "    theme_data = pickle.load(theme_file)\n",
    "    print(len(theme_data))\n",
    "    theme_file.close()\n",
    "    \n",
    "    theme_test_file = open(\"LDA theme/\" + project_name + \"_theme_test.pkl\", \"rb\")\n",
    "    theme_test_data = pickle.load(theme_test_file)\n",
    "    print(len(theme_test_data))\n",
    "    theme_test_file.close()\n",
    "    \n",
    "    \n",
    "    train_valid_data = data[:int(len(data) * 0.9)]\n",
    "    \n",
    "    \n",
    "    print(len(train_valid_data))\n",
    "    print(len(theme_data))\n",
    "    \n",
    "    for i in range(len(train_valid_data)):\n",
    "        if i == 0: print(train_valid_data[0], theme_data[i])\n",
    "        train_valid_data[i].insert(4, theme_data[i])\n",
    "        \n",
    "    train_valid_data = shuffle(train_valid_data, random_state=0)\n",
    "    train_data = train_valid_data[:int(len(train_valid_data) * 0.9)]\n",
    "    valid_data = train_valid_data[int(len(train_valid_data) * 0.9):]\n",
    "    test_data = data[int(len(data) * 0.9):]\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        test_data[i].insert(4, theme_test_data[i])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(train_valid_data[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('train:',len(train_data))\n",
    "    print('validation', len(valid_data))\n",
    "    print('test', len(test_data))\n",
    "    pickle.dump(train_data, open(project_dir + \"/train_nn.pkl\", 'wb'))\n",
    "    pickle.dump(valid_data, open(project_dir + \"/valid_nn.pkl\", 'wb'))\n",
    "    pickle.dump(test_data, open(project_dir + \"/test_nn.pkl\", 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all():\n",
    "\n",
    "    all_dir = output_dir + '\\\\' + 'all'\n",
    "    if not os.path.exists(all_dir):\n",
    "        os.makedirs(all_dir)\n",
    "\n",
    "    word2index_summary = {}  # word: index\n",
    "    word2index_description = {}\n",
    "    index_label = []\n",
    "    index_summary = 1  # 0 used for padding\n",
    "    index_description = 1\n",
    "    print('t1')\n",
    "\n",
    "    for project_name in [\"jackrabbit\", \"lucene\", \"httpclient\"]:\n",
    "        print('t2')\n",
    "        input_file = open(\"dataset/\" + project_name + \".pkl\", \"rb\")\n",
    "        info = pickle.load(input_file)\n",
    "        input_file.close()\n",
    "\n",
    "        key = []\n",
    "        summary = []\n",
    "        description = []\n",
    "        priority = []\n",
    "        for i in range(len(info)):\n",
    "            key.append(info[i]['key'])\n",
    "            summary.append(info[i]['fields']['summary'])\n",
    "            description.append(info[i]['fields']['description'])\n",
    "            priority.append(info[i]['fields']['priority']['id'])\n",
    "\n",
    "        # 将summary、description分别分词、去停用词\n",
    "        summary_tokenize = []\n",
    "        description_tokenize = []\n",
    "        for i in range(len(summary)):\n",
    "            cur_summary = summary[i]\n",
    "            cur_description = description[i]\n",
    "            summary_words = list(word_tokenize(cur_summary))\n",
    "            summary_words = [word.lower() for word in summary_words]\n",
    "            summary_words = remove_stop_words(summary_words, stop_words_path)\n",
    "            summary_words = stem_words(summary_words)\n",
    "            try:\n",
    "                description_words = list(word_tokenize(cur_description))\n",
    "                description_words = [word.lower() for word in description_words]\n",
    "                description_words = remove_stop_words(description_words, stop_words_path)\n",
    "                description_words = stem_words(description_words)\n",
    "            except Exception:\n",
    "                description_words = []\n",
    "            summary_tokenize.append(summary_words)\n",
    "            description_tokenize.append(description_words)\n",
    "        summary_processed = np.array(summary_tokenize)\n",
    "        description_processed = np.array(description_tokenize)\n",
    "\n",
    "        info = pd.read_csv(\"dataset/\" + project_name + \"_classification_vs_type.csv\")\n",
    "        label = list((info['CLASSIFIED'] == \"BUG\").astype(int))\n",
    "\n",
    "        for i in range(len(summary_processed)):\n",
    "            summary_index = []\n",
    "            description_index = []\n",
    "\n",
    "            for word in summary_processed[i]:\n",
    "                if word not in word2index_summary:\n",
    "                    word2index_summary[word] = index_summary\n",
    "                    summary_index.append(index_summary)\n",
    "                    index_summary += 1\n",
    "                else:\n",
    "                    summary_index.append(word2index_summary[word])\n",
    "\n",
    "            for word in description_processed[i]:\n",
    "                if word not in word2index_description:\n",
    "                    word2index_description[word] = index_description\n",
    "                    description_index.append(index_description)\n",
    "                    index_description += 1\n",
    "                else:\n",
    "                    description_index.append(word2index_description[word])\n",
    "\n",
    "            index_label.append([key[i], summary_index, description_index, priority[i], label[i]])\n",
    "\n",
    "    # 将word2index、index_label写入文件\n",
    "    pickle.dump(word2index_summary, open(all_dir + \"/word2index_summary.pkl\", 'wb'))\n",
    "    pickle.dump(word2index_description, open(all_dir + \"/word2index_description.pkl\", 'wb'))\n",
    "    pickle.dump(index_label, open(all_dir + \"/index_label.pkl\", 'wb'))\n",
    "\n",
    "    # 对句向量进行截取或填充\n",
    "    pad_forward(all_dir + \"/index_label.pkl\", all_dir + \"/index_label_nn.pkl\",\n",
    "                summary_len=50, description_len=100)\n",
    "\n",
    "    # 将整理好的数据划分为训练集、验证集、测试集\n",
    "    input_file = open(all_dir + \"/index_label_nn.pkl\", \"rb\")\n",
    "    data = pickle.load(input_file)\n",
    "    input_file.close()\n",
    "\n",
    "    info = []\n",
    "    for project_name in ['jackrabbit', 'lucene', 'httpclient']:\n",
    "        input_file = open(\"dataset/\" + project_name + \".pkl\", \"rb\")\n",
    "        info += pickle.load(input_file)\n",
    "        input_file.close()\n",
    "\n",
    "    key_id = []\n",
    "    for i in range(len(info)):\n",
    "        key_id.append(info[i]['id'])\n",
    "    key_id = np.array(key_id).astype(int)\n",
    "    indicies = np.argsort(key_id)\n",
    "\n",
    "    train_valid_indicies = indicies[:int(len(indicies) * 0.9)]\n",
    "    test_indicies = indicies[int(len(indicies) * 0.9):]\n",
    "\n",
    "    train_valid_data = [data[i] for i in train_valid_indicies]\n",
    "    test_data = [data[i] for i in test_indicies]\n",
    "\n",
    "    train_valid_data = shuffle(train_valid_data, random_state=0)\n",
    "    train_data = train_valid_data[:int(len(train_valid_data) * 0.95)]\n",
    "    valid_data = train_valid_data[int(len(train_valid_data) * 0.95):]\n",
    "\n",
    "    pickle.dump(train_data, open(all_dir + \"/train_nn.pkl\", 'wb'))\n",
    "    pickle.dump(valid_data, open(all_dir + \"/valid_nn.pkl\", 'wb'))\n",
    "    pickle.dump(test_data, open(all_dir + \"/test_nn.pkl\", 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess jackrabbit\n",
      "3038\n",
      "17336\n",
      "2402\n",
      "pad_forward Model Input\\jackrabbit/index_label.pkl\n",
      "2161\n",
      "241\n",
      "2161\n",
      "2161\n",
      "['JCR-2', [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 7, 19, 20, 21, 22, 23, 16, 1, 24, 25, 11, 21, 26, 27, 28, 29, 28, 30, 28, 31, 28, 32, 28, 33, 28, 34, 35, 7, 36, 21, 37, 30, 38, 29, 39, 40, 41, 42, 7, 43, 23, 44, 45, 46, 47, 26, 48, 49, 1, 50, 51, 52, 7, 53, 28, 23, 54, 55, 56, 57, 51, 58, 35, 59, 60, 61, 62, 26, 63, 12, 64, 1, 56, 7, 65, 23, 66, 1, 67, 68, 69, 70], '4', 0] [0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "['JCR-210', [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 260, 1, 344, 510, 345, 313, 42], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1503, 13, 1222, 15, 89, 1378, 182, 2377, 637], '4', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0]\n",
      "train: 1944\n",
      "validation 217\n",
      "test 241\n",
      "preprocess lucene\n",
      "3385\n",
      "18619\n",
      "2443\n",
      "pad_forward Model Input\\lucene/index_label.pkl\n",
      "2198\n",
      "245\n",
      "2198\n",
      "2198\n",
      "['LUCENE-202', [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 5, 9, 10, 7, 11, 12, 2, 3, 13, 14, 15, 16, 17, 9, 18, 6, 2, 3, 4, 6, 19, 2, 20, 6, 2, 21, 3, 3, 7, 22, 23, 5, 9, 24, 10, 25, 26, 27, 3, 28, 29, 30, 31, 32, 9, 33, 34, 6, 35, 4, 36, 9], '3', 0] [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "['LUCENE-1180', [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1295, 11], [0, 0, 0, 0, 0, 127, 6160, 110, 5459, 26, 3926, 131, 827, 1018, 1155, 84, 9, 6161, 2, 6162, 3, 6163, 2, 6164, 3, 6165, 2, 6166, 3, 117, 179, 6167, 63, 1229, 1674, 2, 6168, 2, 3, 988, 1018, 3, 76, 1945, 2, 6169, 2, 3, 988, 1018, 3, 76, 321, 282, 6170, 90, 4, 1265, 7, 4, 1036, 7, 52, 442, 207, 9, 636, 215, 219, 1863, 70, 9, 779, 1229, 212, 665, 6171, 7, 14, 159, 1229, 133, 9, 127, 7, 6160, 1208, 357, 811, 9, 132, 1300, 6172, 45, 7, 487, 78, 94, 8, 9], '4', [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], 1]\n",
      "train: 1978\n",
      "validation 220\n",
      "test 245\n",
      "preprocess httpclient\n",
      "1377\n",
      "7985\n",
      "746\n",
      "pad_forward Model Input\\httpclient/index_label.pkl\n",
      "671\n",
      "75\n",
      "671\n",
      "671\n",
      "['HTTPCLIENT-3', [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 4, 10, 6, 7, 11, 12, 13, 7, 6, 14, 15, 16, 17, 10, 18, 19, 20, 21, 22, 23, 24, 25, 10], '3', 1] [1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n",
      "['HTTPCLIENT-1028', [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 469, 1243, 1244], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4892, 7190, 5, 7191, 324, 7192, 3292, 23, 242, 1045, 7191, 1562, 242, 612, 7193, 10, 884, 5, 7191, 3292, 5384, 1045, 1018, 1562, 5384, 612, 7193, 10], '3', [1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], 0]\n",
      "train: 603\n",
      "validation 68\n",
      "test 75\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"preprocess jackrabbit\")\n",
    "    preprocess_project(\"jackrabbit\")\n",
    "    print(\"preprocess lucene\")\n",
    "    preprocess_project(\"lucene\")\n",
    "    print(\"preprocess httpclient\")\n",
    "    preprocess_project(\"httpclient\")\n",
    "#     print(\"preprocess all\")\n",
    "#     preprocess_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
