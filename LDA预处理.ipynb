{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gensim\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#分别对summary和description建立词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'LDA-Input'\n",
    "stop_words_path = 'dataset/english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(inputs, stop_words_path):\n",
    "    '''\n",
    "    去除停用词\n",
    "    :param inputs: [word1 word2...]\n",
    "    :param stop_words_path:\n",
    "    :return:\n",
    "    '''\n",
    "    with open(stop_words_path, \"r\", encoding=\"utf-8\") as fr:\n",
    "        stop_words = [line.strip() for line in fr.readlines()]\n",
    "\n",
    "    outputs = [word for word in inputs if word not in stop_words]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(inputs):\n",
    "    '''\n",
    "    词干化处理\n",
    "    :param inputs: [word1 word2...]\n",
    "    :return:\n",
    "    '''\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    # stemmer = PorterStemmer()\n",
    "    outputs = [stemmer.stem(word) for word in inputs]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(input):\n",
    "    punctuations = ['!', '\"', '#', '$', \"'\", \n",
    "                    '%', '&', '(', ')', '*', \n",
    "                    '+', ',', '-', '.', '/', \n",
    "                    ':', ';', '<', '=', '>', \n",
    "                    '?', '@', '[', '\\\\', ']',\n",
    "                    '^', '_', '`', '{', '|', \n",
    "                    '}', '~', \"``\", \"--\", \"...\",\n",
    "                   \"''\", ]\n",
    "    output = [word for word in input if word not in punctuations]\n",
    "    \n",
    "    pattern_1 = re.compile(\"^[=/*\\\\.\\s]+$\")\n",
    "    for i in range(len(output)):\n",
    "        if pattern_1.match(output[i]) is not None:\n",
    "            output[i] = \"<PUNCTUTATION>\"\n",
    "            continue\n",
    "    output = [word for word in output if word!=\"<PUNCTUTATION>\"]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_project(project_name):\n",
    "\n",
    "    project_dir = output_dir + '\\\\' + project_name\n",
    "    if not os.path.exists(project_dir):\n",
    "        os.makedirs(project_dir)\n",
    "\n",
    "    #读取pkl文件\n",
    "    input_file = open(\"dataset/\" + project_name + \".pkl\", \"rb\")\n",
    "    info = pickle.load(input_file)\n",
    "    input_file.close()\n",
    "\n",
    "    #处理数据\n",
    "    key = []\n",
    "    key_id = []\n",
    "    summary = []\n",
    "    description = []\n",
    "    priority = []\n",
    "    for i in range(len(info)):\n",
    "        key.append(info[i]['key']) #'HTTPCLIENT-569'\n",
    "        cur_index = info[i]['key'].find(\"-\") #10\n",
    "        key_id.append(int(info[i]['key'][cur_index + 1:])) #569\n",
    "        summary.append(info[i]['fields']['summary'])\n",
    "        description.append(info[i]['fields']['description'])\n",
    "        priority.append(info[i]['fields']['priority']['id'])\n",
    "    key = np.array(key)\n",
    "    key_id = np.array(key_id)\n",
    "    summary = np.array(summary)\n",
    "    description = np.array(description)\n",
    "    priority = np.array(priority)\n",
    "    \"\"\"\n",
    "    key: 软件缺陷报告的key\n",
    "    key_id：软件缺陷报告的key_id\n",
    "    summary：软件缺陷报告的摘要\n",
    "    description：件缺陷报告的描述\n",
    "    priority：软件缺陷报告的优先级\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"将所有元素按照key_id升序进行排序\"\"\"\n",
    "    indicies = np.argsort(key_id) #元素从小到大排列后提取索引\n",
    "    key = key[indicies]\n",
    "    summary = summary[indicies]\n",
    "    description = description[indicies]\n",
    "    priority = priority[indicies]\n",
    "\n",
    "    #将summary、description分别分词、去停用词\n",
    "    summary_tokenize = []\n",
    "    description_tokenize = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(len(summary)):\n",
    "        cur_summary = summary[i]\n",
    "        cur_description = description[i]\n",
    "        \"\"\"处理摘要\"\"\"\n",
    "        summary_words = list(word_tokenize(cur_summary)) # 分词\n",
    "        summary_words = [word.lower() for word in summary_words] # 转换为小写\n",
    "        summary_words = remove_stop_words(summary_words, stop_words_path) # 去除停用词\n",
    "        summary_words = stem_words(summary_words) # 词干化\n",
    "        summary_words = remove_punctuation(summary_words) # 去除标点\n",
    "        try:\n",
    "            \"\"\"处理描述\"\"\"\n",
    "            description_words = list(word_tokenize(cur_description)) # 分词\n",
    "            description_words = [word.lower() for word in description_words] # 转换为小写\n",
    "            description_words = remove_stop_words(description_words, stop_words_path) # 去除停用词\n",
    "            description_words = stem_words(description_words) # 词干化\n",
    "            description_words = remove_punctuation(description_words) # 去除标点\n",
    "        except Exception:\n",
    "            description_words = []\n",
    "        \"\"\"将处理后的摘要和描述放入切分词列表\"\"\"\n",
    "        summary_tokenize.append(summary_words)\n",
    "        description_tokenize.append(description_words)\n",
    "        \n",
    "        \n",
    "    summary_processed = np.array(summary_tokenize)\n",
    "    description_processed = np.array(description_tokenize)\n",
    "\n",
    "#     print(summary_processed)\n",
    "#     print(description_processed)\n",
    "    print(len(summary_processed))\n",
    "    print(len(description_processed))\n",
    "    all_report = []\n",
    "    for i in range(len(summary_processed)):\n",
    "        if i==0: print(summary_processed[i])\n",
    "        summary_processed[i].extend(description_processed[i])\n",
    "        all_report.append(summary_processed[i])\n",
    "        if i==0: print(summary_processed[i])\n",
    "    \n",
    "    print(len(all_report))\n",
    "    train_valid_report = all_report[:int(len(all_report) * 0.9)]\n",
    "    test_report = all_report[int(len(all_report) * 0.9):]\n",
    "    \n",
    " \n",
    "    fp = open('LDA-Input\\\\lucene-train.txt','w+',encoding='utf-8')\n",
    "    fp.write(str(len(train_valid_report))+'\\n')\n",
    "    for i in range(len(train_valid_report)):\n",
    "        fp.write(\" \".join(train_valid_report[i]))\n",
    "        if i != len(train_valid_report)-1: \n",
    "            fp.write(\"\\n\") \n",
    "    fp.close()\n",
    "    \n",
    "    fp = open('LDA-Input\\\\lucene-test.txt','w+',encoding='utf-8')\n",
    "    fp.write(str(len(test_report))+'\\n')\n",
    "    for i in range(len(test_report)):\n",
    "        fp.write(\" \".join(test_report[i]))\n",
    "        if i != len(test_report)-1: \n",
    "            fp.write(\"\\n\") \n",
    "    fp.close()\n",
    "\n",
    "#     #读取label并整理 classified为bug的值为1，否则为0\n",
    "#     info = pd.read_csv(\"dataset/\" + project_name + \"_classification_vs_type.csv\")\n",
    "#     label = list((info['CLASSIFIED'] == \"BUG\").astype(int))\n",
    "#     label = np.array(label)\n",
    "#     label = label[indicies]\n",
    "\n",
    "#     word2index_summary = {} # word: index\n",
    "#     word2index_description = {} # 单词到索引的映射\n",
    "    \n",
    "#     index_label = [] #[key,[summary_index...],[description_index...],label]\n",
    "#     index_summary = 1  # 0 used for padding\n",
    "#     index_description = 1\n",
    "#     for i in range(len(summary_processed)):\n",
    "#         summary_index = []\n",
    "#         description_index = []\n",
    "\n",
    "#         for word in summary_processed[i]:\n",
    "#             if word not in word2index_summary:\n",
    "#                 word2index_summary[word] = index_summary\n",
    "#                 summary_index.append(index_summary)\n",
    "#                 index_summary += 1\n",
    "#             else:\n",
    "#                 summary_index.append(word2index_summary[word])\n",
    "\n",
    "#         for word in description_processed[i]:\n",
    "#             if word not in word2index_description:\n",
    "#                 word2index_description[word] = index_description\n",
    "#                 description_index.append(index_description)\n",
    "#                 index_description += 1\n",
    "#             else:\n",
    "#                 description_index.append(word2index_description[word])\n",
    "\n",
    "#         index_label.append([key[i], summary_index, description_index, priority[i], label[i]])\n",
    "#     for key in word2index_description:\n",
    "#         print(key,\":\", word2index_description[key])\n",
    "\n",
    "#     print(len(word2index_summary))\n",
    "#     print(len(word2index_description))\n",
    "#     print(len(index_label))\n",
    "    #将word2index、index_label写入文件\n",
    "#     pickle.dump(word2index_summary, open(project_dir + \"/word2index_summary.pkl\", 'wb'))\n",
    "#     pickle.dump(word2index_description, open(project_dir + \"/word2index_description.pkl\", 'wb'))\n",
    "#     pickle.dump(index_label, open(project_dir + \"/index_label.pkl\", 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2443\n",
      "2443\n",
      "['document.field', 'return', 'store', 'field']\n",
      "['document.field', 'return', 'store', 'field', 'document.field', 'return', 'store', 'field', 'index', 'store', 'confus', \"'s\", 'isstor', 'method', \"n't\", 'make', 'much', 'sens', 'actual', 'field', 'return', 'field', 'document.add', 'new', 'field', 'even', 'one', 'store', 'sound', 'confus', 'll', 'attach', 'small', 'program', 'demonstr', 'either', 'fix', 'field', 'alway', 'return', 'document']\n",
      "2443\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    preprocess_project('lucene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
