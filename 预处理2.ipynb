{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gensim\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'output4'\n",
    "stop_words_path = './dataset/english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(inputs, stop_words_path):\n",
    "    '''\n",
    "    去除停用词\n",
    "    :param inputs: [word1 word2...]\n",
    "    :param stop_words_path:\n",
    "    :return:\n",
    "    '''\n",
    "    with open(stop_words_path, \"r\", encoding=\"utf-8\") as fr:\n",
    "        stop_words = [line.strip() for line in fr.readlines()]\n",
    "    outputs = [word for word in inputs if word not in stop_words]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(inputs):\n",
    "    '''\n",
    "    词干化处理\n",
    "    :param inputs: [word1 word2...]\n",
    "    :return:\n",
    "    '''\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    # stemmer = PorterStemmer()\n",
    "    outputs = [stemmer.stem(word) for word in inputs]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_forward(in_file, out_file, summary_len, description_len):\n",
    "    '''对词向量进行截取或者填充'''\n",
    "    print(\"pad_forward\", in_file)\n",
    "    nn_data = []\n",
    "    for data in pickle.load(open(in_file, 'rb')):\n",
    "        summary_sent = data[1]\n",
    "        description_sent = data[2]\n",
    "        if len(summary_sent) >= summary_len:\n",
    "            summary_sent = summary_sent[:summary_len]\n",
    "        else:\n",
    "            pad = [0] * (summary_len - len(summary_sent))\n",
    "            summary_sent = pad + summary_sent\n",
    "\n",
    "        if len(description_sent) >= description_len:\n",
    "            description_sent = description_sent[:description_len]\n",
    "        else:\n",
    "            pad = [0] * (description_len - len(description_sent))\n",
    "            description_sent = pad + description_sent\n",
    "        nn_data.append([data[0], summary_sent, description_sent, data[3], data[4]])\n",
    "    pickle.dump(nn_data, open(out_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_project(project_name):\n",
    "\n",
    "    project_dir = output_dir + '\\\\' + project_name\n",
    "    if not os.path.exists(project_dir):\n",
    "        os.makedirs(project_dir)\n",
    "\n",
    "    #读取pkl文件\n",
    "    input_file = open(\"dataset/\" + project_name + \".pkl\", \"rb\")\n",
    "    info = pickle.load(input_file)\n",
    "    input_file.close()\n",
    "\n",
    "    #处理数据\n",
    "    key = []\n",
    "    key_id = []\n",
    "    summary = []\n",
    "    description = []\n",
    "    priority = []\n",
    "    for i in range(len(info)):\n",
    "        key.append(info[i]['key']) #'HTTPCLIENT-569'\n",
    "        cur_index = info[i]['key'].find(\"-\") #10\n",
    "        key_id.append(int(info[i]['key'][cur_index + 1:])) #569\n",
    "        #'HttpState.clearCookies() should be synchronized'\n",
    "        summary.append(info[i]['fields']['summary'])\n",
    "        \"\"\"\n",
    "        ('The HttpState class has a clearCookies method that is not synchronized but\\n'\n",
    "         'should be considering it modifies an ArrayList (which is unsynchronized). '\n",
    "         'All\\n'\n",
    "         'other methods which modify or read from the ArrayList are synchronized '\n",
    "         'except\\n'\n",
    "         'the clearCookies method. \\n'\n",
    "         '\\n'\n",
    "         'I stumbled upon this fact because a webapp I am working on that uses '\n",
    "         'HttpClient\\n'\n",
    "         'threw an IllegalArgumentException indicating that one of the cookies in the\\n'\n",
    "         \"array returned from HttpState.getCookies() was null, which shouldn't be\\n\"\n",
    "         'possible.  Upon further inspection and testing, the only possible option is '\n",
    "         'that\\n'\n",
    "         'the threadsafety hole left by the unsynchronized clearCookies method caused '\n",
    "         'the\\n'\n",
    "         'issue.')\n",
    "        \"\"\"\n",
    "        description.append(info[i]['fields']['description'])\n",
    "        priority.append(info[i]['fields']['priority']['id'])\n",
    "    key = np.array(key)\n",
    "    key_id = np.array(key_id)\n",
    "    summary = np.array(summary)\n",
    "    description = np.array(description)\n",
    "    priority = np.array(priority)\n",
    "\n",
    "    indicies = np.argsort(key_id) #元素从小到大排列后提取索引\n",
    "    key = key[indicies]\n",
    "    summary = summary[indicies]\n",
    "    description = description[indicies]\n",
    "    priority = priority[indicies]\n",
    "\n",
    "    #将summary、description分别分词、去停用词\n",
    "    summary_tokenize = []\n",
    "    description_tokenize = []\n",
    "    for i in range(len(summary)):\n",
    "        cur_summary = summary[i]\n",
    "        cur_description = description[i]\n",
    "        summary_words = list(word_tokenize(cur_summary))\n",
    "        summary_words = [word.lower() for word in summary_words]\n",
    "        summary_words = remove_stop_words(summary_words, stop_words_path)\n",
    "        summary_words = stem_words(summary_words)\n",
    "        try:\n",
    "            description_words = list(word_tokenize(cur_description))\n",
    "            description_words = [word.lower() for word in description_words]\n",
    "            description_words = remove_stop_words(description_words, stop_words_path)\n",
    "            description_words = stem_words(description_words)\n",
    "        except Exception:\n",
    "            description_words = []\n",
    "        summary_tokenize.append(summary_words)\n",
    "        description_tokenize.append(description_words)\n",
    "    summary_tokenize = np.array(summary_tokenize)\n",
    "    description_tokenize = np.array(description_tokenize)\n",
    "\n",
    "    #将经过分词、去停用词后的summary、description中的数字和“.”置换为“<NUM>” here error\n",
    "    pattern1 = re.compile(\"^[0-9\\.]+$\")\n",
    "    summary_processed = []\n",
    "    for i in range(len(summary_tokenize)):\n",
    "        cur_summary_tokenize = []\n",
    "        for word in summary_tokenize[i]:\n",
    "            if pattern1.match(word) is not None:\n",
    "                cur_summary_tokenize.append(\"<NUM>\")\n",
    "                continue\n",
    "            cur_summary_tokenize.append(word)\n",
    "        summary_processed.append(cur_summary_tokenize)\n",
    "\n",
    "    description_processed = []\n",
    "    for i in range(len(description_tokenize)):\n",
    "        cur_description_tokenize = []\n",
    "        for word in description_tokenize[i]:\n",
    "            if pattern1.match(word) is not None:\n",
    "                cur_description_tokenize.append(\"<NUM>\")\n",
    "                continue\n",
    "            cur_description_tokenize.append(word)\n",
    "        description_processed.append(cur_description_tokenize)\n",
    "\n",
    "    summary_processed = np.array(summary_processed)\n",
    "    description_processed = np.array(description_processed)\n",
    "\n",
    "\n",
    "    #读取label并整理 classified为bug的值为1，否则为0\n",
    "    info = pd.read_csv(\"dataset/\" + project_name + \"_classification_vs_type.csv\")\n",
    "    label = list((info['CLASSIFIED'] == \"BUG\").astype(int))\n",
    "    label = np.array(label)\n",
    "    label = label[indicies]\n",
    "\n",
    "    word2index = {} #word: index\n",
    "    index_label = [] #[key,[summary_index...],[description_index...],label]\n",
    "    index = 1  # 0 used for padding\n",
    "    for i in range(len(summary_processed)):\n",
    "        summary_index = []\n",
    "        description_index = []\n",
    "\n",
    "        for word in summary_processed[i]:\n",
    "            if word not in word2index:\n",
    "                word2index[word] = index\n",
    "                summary_index.append(index)\n",
    "                index += 1\n",
    "            else:\n",
    "                summary_index.append(word2index[word])\n",
    "\n",
    "        for word in description_processed[i]:\n",
    "            if word not in word2index:\n",
    "                word2index[word] = index\n",
    "                description_index.append(index)\n",
    "                index += 1\n",
    "            else:\n",
    "                description_index.append(word2index[word])\n",
    "\n",
    "        index_label.append([key[i], summary_index, description_index, priority[i], label[i]])\n",
    "\n",
    "    #将word2index、index_label写入文件\n",
    "    pickle.dump(word2index, open(project_dir + \"/word2index.pkl\", 'wb'))\n",
    "    pickle.dump(index_label, open(project_dir + \"/index_label.pkl\", 'wb'))\n",
    "\n",
    "    # 对句向量进行截取或填充\n",
    "    pad_forward(project_dir + \"/index_label.pkl\", project_dir + \"/index_label_nn.pkl\",\n",
    "                summary_len=50, description_len=100)\n",
    "\n",
    "    #将整理好的数据划分为训练集、验证集、测试集\n",
    "    input_file = open(project_dir + \"/index_label_nn.pkl\", \"rb\")\n",
    "    data = pickle.load(input_file)\n",
    "    input_file.close()\n",
    "    train_valid_data = data[:int(len(data) * 0.9)]\n",
    "    train_valid_data = shuffle(train_valid_data, random_state=0)\n",
    "    train_data = train_valid_data[:int(len(train_valid_data) * 0.9)]\n",
    "    valid_data = train_valid_data[int(len(train_valid_data) * 0.9):]\n",
    "    test_data = data[int(len(data) * 0.9):]\n",
    "    pickle.dump(train_data, open(project_dir + \"/train_nn.pkl\", 'wb'))\n",
    "    pickle.dump(valid_data, open(project_dir + \"/valid_nn.pkl\", 'wb'))\n",
    "    pickle.dump(test_data, open(project_dir + \"/test_nn.pkl\", 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all():\n",
    "\n",
    "    all_dir = output_dir + '\\\\' + 'all'\n",
    "    if not os.path.exists(all_dir):\n",
    "        os.makedirs(all_dir)\n",
    "\n",
    "    word2index = {}\n",
    "    index_label = []\n",
    "    index = 1  # 0 used for padding\n",
    "    print('t1')\n",
    "\n",
    "    for project_name in [\"jackrabbit\", \"lucene\", \"httpclient\"]:\n",
    "        print('t2')\n",
    "        input_file = open(\"dataset/\" + project_name + \".pkl\", \"rb\")\n",
    "        info = pickle.load(input_file)\n",
    "        input_file.close()\n",
    "\n",
    "        key = []\n",
    "        summary = []\n",
    "        description = []\n",
    "        priority = []\n",
    "        for i in range(len(info)):\n",
    "            key.append(info[i]['key'])\n",
    "            summary.append(info[i]['fields']['summary'])\n",
    "            description.append(info[i]['fields']['description'])\n",
    "            priority.append(info[i]['fields']['priority']['id'])\n",
    "\n",
    "        # 将summary、description分别分词、去停用词\n",
    "        summary_tokenize = []\n",
    "        description_tokenize = []\n",
    "        for i in range(len(summary)):\n",
    "            cur_summary = summary[i]\n",
    "            cur_description = description[i]\n",
    "            summary_words = list(word_tokenize(cur_summary))\n",
    "            summary_words = [word.lower() for word in summary_words]\n",
    "            summary_words = remove_stop_words(summary_words, stop_words_path)\n",
    "            summary_words = stem_words(summary_words)\n",
    "            try:\n",
    "                description_words = list(word_tokenize(cur_description))\n",
    "                description_words = [word.lower() for word in description_words]\n",
    "                description_words = remove_stop_words(description_words, stop_words_path)\n",
    "                description_words = stem_words(description_words)\n",
    "            except Exception:\n",
    "                description_words = []\n",
    "            summary_tokenize.append(summary_words)\n",
    "            description_tokenize.append(description_words)\n",
    "        summary_tokenize = np.array(summary_tokenize)\n",
    "        description_tokenize = np.array(description_tokenize)\n",
    "\n",
    "        # 将经过分词、去停用词后的summary、description中的数字和“.”置换为“<NUM>” here error\n",
    "        pattern1 = re.compile(\"^[0-9\\.]+$\")\n",
    "        summary_processed = []\n",
    "        for i in range(len(summary_tokenize)):\n",
    "            cur_summary_tokenize = []\n",
    "            for word in summary_tokenize[i]:\n",
    "                if pattern1.match(word) is not None:\n",
    "                    cur_summary_tokenize.append(\"<NUM>\")\n",
    "                    continue\n",
    "                cur_summary_tokenize.append(word)\n",
    "            summary_processed.append(cur_summary_tokenize)\n",
    "\n",
    "        description_processed = []\n",
    "        for i in range(len(description_tokenize)):\n",
    "            cur_description_tokenize = []\n",
    "            for word in description_tokenize[i]:\n",
    "                if pattern1.match(word) is not None:\n",
    "                    cur_description_tokenize.append(\"<NUM>\")\n",
    "                    continue\n",
    "                cur_description_tokenize.append(word)\n",
    "            description_processed.append(cur_description_tokenize)\n",
    "\n",
    "        summary_processed = np.array(summary_processed)\n",
    "        description_processed = np.array(description_processed)\n",
    "\n",
    "        info = pd.read_csv(\"dataset/\" + project_name + \"_classification_vs_type.csv\")\n",
    "        label = list((info['CLASSIFIED'] == \"BUG\").astype(int))\n",
    "\n",
    "        for i in range(len(summary_processed)):\n",
    "            summary_index = []\n",
    "            description_index = []\n",
    "\n",
    "            for word in summary_processed[i]:\n",
    "                if word not in word2index:\n",
    "                    word2index[word] = index\n",
    "                    summary_index.append(index)\n",
    "                    index += 1\n",
    "                else:\n",
    "                    summary_index.append(word2index[word])\n",
    "\n",
    "            for word in description_processed[i]:\n",
    "                if word not in word2index:\n",
    "                    word2index[word] = index\n",
    "                    description_index.append(index)\n",
    "                    index += 1\n",
    "                else:\n",
    "                    description_index.append(word2index[word])\n",
    "\n",
    "            index_label.append([key[i], summary_index, description_index, priority[i], label[i]])\n",
    "\n",
    "    # 将word2index、index_label写入文件\n",
    "    pickle.dump(word2index, open(all_dir + \"/word2index.pkl\", 'wb'))\n",
    "    pickle.dump(index_label, open(all_dir + \"/index_label.pkl\", 'wb'))\n",
    "\n",
    "    # 对句向量进行截取或填充\n",
    "    pad_forward(all_dir + \"/index_label.pkl\", all_dir + \"/index_label_nn.pkl\",\n",
    "                summary_len=50, description_len=100)\n",
    "\n",
    "    # 将整理好的数据划分为训练集、验证集、测试集\n",
    "    input_file = open(all_dir + \"/index_label_nn.pkl\", \"rb\")\n",
    "    data = pickle.load(input_file)\n",
    "    input_file.close()\n",
    "\n",
    "    info = []\n",
    "    for project_name in ['jackrabbit', 'lucene', 'httpclient']:\n",
    "        input_file = open(\"dataset/\" + project_name + \".pkl\", \"rb\")\n",
    "        info += pickle.load(input_file)\n",
    "        input_file.close()\n",
    "\n",
    "    key_id = []\n",
    "    for i in range(len(info)):\n",
    "        key_id.append(info[i]['id'])\n",
    "    key_id = np.array(key_id).astype(int)\n",
    "    indicies = np.argsort(key_id)\n",
    "\n",
    "    train_valid_indicies = indicies[:int(len(indicies) * 0.9)]\n",
    "    test_indicies = indicies[int(len(indicies) * 0.9):]\n",
    "\n",
    "    train_valid_data = [data[i] for i in train_valid_indicies]\n",
    "    test_data = [data[i] for i in test_indicies]\n",
    "\n",
    "    train_valid_data = shuffle(train_valid_data, random_state=0)\n",
    "    train_data = train_valid_data[:int(len(train_valid_data) * 0.95)]\n",
    "    valid_data = train_valid_data[int(len(train_valid_data) * 0.95):]\n",
    "\n",
    "    pickle.dump(train_data, open(all_dir + \"/train_nn.pkl\", 'wb'))\n",
    "    pickle.dump(valid_data, open(all_dir + \"/valid_nn.pkl\", 'wb'))\n",
    "    pickle.dump(test_data, open(all_dir + \"/test_nn.pkl\", 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "746\n",
      "305\n",
      "441\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#     print(\"preprocess jackrabbit\")\n",
    "#     preprocess_project(\"jackrabbit\")\n",
    "#     print(\"preprocess lucene\")\n",
    "#     preprocess_project(\"lucene\")\n",
    "#     print(\"preprocess httpclient\")\n",
    "#     preprocess_project(\"httpclient\")\n",
    "#     print(\"preprocess all\")\n",
    "#     preprocess_all()\n",
    "\n",
    "#     inputs = ['i', 'me', 'apple', 'you', 'orange', 'yourself', '?', 'oranges']\n",
    "#     outputs = remove_stop_words(inputs, stop_words_path)\n",
    "#     outputs = stem_words(outputs)\n",
    "#     print(outputs)\n",
    "\n",
    "#     cur_summary = \"HttpState.clearCookies() should be synchronized\"\n",
    "#     summary_words = list(word_tokenize(cur_summary))\n",
    "#     print(summary_words)\n",
    "\n",
    "    project_name = \"httpclient\"\n",
    "    info = pd.read_csv(\"dataset/\" + project_name + \"_classification_vs_type.csv\")\n",
    "    label = list((info['CLASSIFIED'] == \"BUG\").astype(int))\n",
    "    label = np.array(label)\n",
    "    number_bug = 0\n",
    "    number_non_bug = 0\n",
    "    print(len(label))\n",
    "    for i in range(len(label)):\n",
    "        if label[i] == 1:\n",
    "            number_bug += 1\n",
    "        elif label[i] == 0:\n",
    "            number_non_bug += 1\n",
    "    \n",
    "    print(number_bug)\n",
    "    print(number_non_bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
